import os
import streamlit as st
import tempfile
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from typing import Any, List, Dict
from datetime import datetime
import logging
import re
import json
from supabase import create_client, Client
from langchain_core.documents import Document
import hashlib

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë¡œì»¬ ê°œë°œìš©)
load_dotenv()

# ë¡œê¹… ì„¤ì •
log_dir = "logs"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

log_filename = os.path.join(log_dir, f"multi_users_{datetime.now().strftime('%Y%m%d')}.log")
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# HTTP ìš”ì²­ ë¡œê·¸ ë¹„í™œì„±í™”
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("langchain").setLevel(logging.WARNING)
logging.getLogger("langchain_openai").setLevel(logging.WARNING)

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def init_supabase():
    """Supabase í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # Streamlit Cloud secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°
    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = os.getenv("SUPABASE_ANON_KEY") or os.getenv("SUPABASE_SERVICE_ROLE_KEY")
    
    if not supabase_url or not supabase_key:
        st.error("SUPABASE_URLê³¼ SUPABASE_ANON_KEY(ë˜ëŠ” SUPABASE_SERVICE_ROLE_KEY)ê°€ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        st.stop()
    
    return create_client(supabase_url, supabase_key)

supabase: Client = init_supabase()

# ë¹„ë°€ë²ˆí˜¸ í•´ì‹œ í•¨ìˆ˜ (ê°„ë‹¨í•œ í•´ì‹œ, í”„ë¡œë•ì…˜ì—ì„œëŠ” bcrypt ë“± ì‚¬ìš© ê¶Œì¥)
def hash_password(password: str) -> str:
    """ë¹„ë°€ë²ˆí˜¸ë¥¼ í•´ì‹œí•©ë‹ˆë‹¤."""
    return hashlib.sha256(password.encode()).hexdigest()

# ì‚¬ìš©ì ì¸ì¦ í•¨ìˆ˜
def authenticate_user(login_id: str, password: str) -> Dict:
    """ì‚¬ìš©ì ì¸ì¦"""
    try:
        hashed_password = hash_password(password)
        response = supabase.table("users").select("*").eq("login_id", login_id).eq("password", hashed_password).execute()
        
        if response.data and len(response.data) > 0:
            return {"success": True, "user": response.data[0]}
        else:
            return {"success": False, "message": "ë¡œê·¸ì¸ ID ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."}
    except Exception as e:
        logger.error(f"ì¸ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"success": False, "message": f"ì¸ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

# ì‚¬ìš©ì ë“±ë¡ í•¨ìˆ˜
def register_user(login_id: str, password: str) -> Dict:
    """ìƒˆ ì‚¬ìš©ì ë“±ë¡"""
    try:
        # ì¤‘ë³µ í™•ì¸
        existing = supabase.table("users").select("*").eq("login_id", login_id).execute()
        if existing.data and len(existing.data) > 0:
            return {"success": False, "message": "ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¡œê·¸ì¸ IDì…ë‹ˆë‹¤."}
        
        # ìƒˆ ì‚¬ìš©ì ìƒì„±
        hashed_password = hash_password(password)
        response = supabase.table("users").insert({
            "login_id": login_id,
            "password": hashed_password
        }).execute()
        
        if response.data and len(response.data) > 0:
            return {"success": True, "user": response.data[0]}
        else:
            return {"success": False, "message": "ì‚¬ìš©ì ë“±ë¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        logger.error(f"ì‚¬ìš©ì ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"success": False, "message": f"ì‚¬ìš©ì ë“±ë¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

# êµ¬ë¶„ì„  ë° ì·¨ì†Œì„  ì œê±° í•¨ìˆ˜
def remove_separators(text: str) -> str:
    """ë‹µë³€ì—ì„œ êµ¬ë¶„ì„ (---, ===, ___)ê³¼ ì·¨ì†Œì„ (~~í…ìŠ¤íŠ¸~~)ì„ ì œê±°í•©ë‹ˆë‹¤."""
    if not text:
        return text
    # ì·¨ì†Œì„  ë§ˆí¬ë‹¤ìš´ ì œê±° (~~í…ìŠ¤íŠ¸~~ -> í…ìŠ¤íŠ¸)
    text = re.sub(r'~~([^~]+)~~', r'\1', text)
    # ì—¬ëŸ¬ ì¤„ì— ê±¸ì¹œ êµ¬ë¶„ì„  ì œê±° (ê³µë°± í¬í•¨)
    text = re.sub(r'\n\s*-{3,}\s*\n', '\n\n', text)
    text = re.sub(r'\n\s*={3,}\s*\n', '\n\n', text)
    text = re.sub(r'\n\s*_{3,}\s*\n', '\n\n', text)
    # ë‹¨ë… ë¼ì¸ì˜ êµ¬ë¶„ì„  ì œê±°
    text = re.sub(r'^\s*-{3,}\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*={3,}\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*_{3,}\s*$', '', text, flags=re.MULTILINE)
    # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬ (ìµœëŒ€ 2ê°œ)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

# LLM ëª¨ë¸ ì„ íƒ í•¨ìˆ˜
def get_llm(model_name: str, temperature: float = 0.7, api_keys: Dict = None) -> Any:
    """ì„ íƒëœ ëª¨ë¸ëª…ì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    api_keys = api_keys or {}
    
    if model_name == "gpt-5.1":
        openai_key = api_keys.get("openai") or os.getenv("OPENAI_API_KEY")
        if not openai_key:
            st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            st.stop()
        return ChatOpenAI(model="gpt-5.1", temperature=temperature, api_key=openai_key)
    elif model_name == "claude-sonnet-4-5":
        from langchain_anthropic import ChatAnthropic
        anthropic_key = api_keys.get("anthropic") or os.getenv("ANTHROPIC_API_KEY")
        if not anthropic_key:
            st.error("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            st.stop()
        return ChatAnthropic(model="claude-sonnet-4-5", temperature=temperature, api_key=anthropic_key)
    elif model_name == "gemini-3-pro-preview":
        from langchain_google_genai import ChatGoogleGenerativeAI
        gemini_key = api_keys.get("gemini") or os.getenv("GOOGLE_API_KEY")
        if not gemini_key:
            st.error("Google API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            st.stop()
        return ChatGoogleGenerativeAI(model="gemini-3-pro-preview", google_api_key=gemini_key, temperature=temperature)
    else:
        # ê¸°ë³¸ê°’: gpt-5.1
        openai_key = api_keys.get("openai") or os.getenv("OPENAI_API_KEY")
        if not openai_key:
            st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            st.stop()
        return ChatOpenAI(model="gpt-5.1", temperature=temperature, api_key=openai_key)

# Supabaseë¥¼ ì‚¬ìš©í•œ ë²¡í„° ìŠ¤í† ì–´ í´ë˜ìŠ¤
class SupabaseVectorStore:
    """Supabaseë¥¼ ë²¡í„° ìŠ¤í† ì–´ë¡œ ì‚¬ìš©í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, session_id: str, embeddings: OpenAIEmbeddings):
        self.session_id = session_id
        self.embeddings = embeddings
        self.supabase = supabase
    
    def add_documents(self, documents: List[Document], file_name: str):
        """ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ì—¬ Supabaseì— ì €ì¥"""
        try:
            # ë¬¸ì„œë¥¼ ì„ë² ë”©
            texts = [doc.page_content for doc in documents]
            embeddings_list = self.embeddings.embed_documents(texts)
            
            # ê° ë¬¸ì„œë¥¼ Supabaseì— ì €ì¥
            for idx, (doc, embedding) in enumerate(zip(documents, embeddings_list)):
                # embeddingì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³ , PostgreSQL vector í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                if isinstance(embedding, list):
                    # ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬ (Supabaseê°€ ìë™ìœ¼ë¡œ vectorë¡œ ë³€í™˜)
                    embedding_value = embedding
                else:
                    embedding_value = list(embedding) if hasattr(embedding, '__iter__') else embedding
                
                data = {
                    "session_id": str(self.session_id),
                    "file_name": file_name,
                    "chunk_index": idx,
                    "content": doc.page_content,
                    "metadata": json.dumps(doc.metadata, ensure_ascii=False),
                    "embedding": embedding_value
                }
                self.supabase.table("document_embeddings").insert(data).execute()
            
            logger.info(f"ë¬¸ì„œ {len(documents)}ê°œë¥¼ Supabaseì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def similarity_search(self, query: str, k: int = 10) -> List[Document]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embeddings.embed_query(query)
            
            # Supabaseì—ì„œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ (cosine distance)
            # PostgreSQLì˜ vector í™•ì¥ ê¸°ëŠ¥ ì‚¬ìš©
            try:
                # RPC í•¨ìˆ˜ íŒŒë¼ë¯¸í„° ìˆœì„œ: query_embedding, match_count, session_id
                rpc_params = {
                    "query_embedding": list(query_embedding) if isinstance(query_embedding, (list, tuple)) else query_embedding,
                    "match_count": k
                }
                if self.session_id:
                    rpc_params["session_id"] = str(self.session_id)
                
                response = self.supabase.rpc("match_documents", rpc_params).execute()
                
                # ê²°ê³¼ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
                documents = []
                if response.data:
                    for row in response.data:
                        metadata_str = row.get("metadata", "{}")
                        if isinstance(metadata_str, str):
                            metadata = json.loads(metadata_str)
                        else:
                            metadata = metadata_str
                        doc = Document(
                            page_content=row.get("content", ""),
                            metadata=metadata
                        )
                        documents.append(doc)
                
                return documents
            except Exception as rpc_error:
                logger.warning(f"RPC í•¨ìˆ˜ í˜¸ì¶œ ì‹¤íŒ¨, ëŒ€ì²´ ê²€ìƒ‰ ì‚¬ìš©: {rpc_error}")
                return self._fallback_search(query, k)
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            # RPC í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ ì§ì ‘ SQL ì¿¼ë¦¬
            return self._fallback_search(query, k)
    
    def _fallback_search(self, query: str, k: int) -> List[Document]:
        """RPC í•¨ìˆ˜ê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ëŒ€ì²´ ê²€ìƒ‰ ë°©ë²•"""
        try:
            import numpy as np
            
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embeddings.embed_query(query)
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            if not isinstance(query_embedding, np.ndarray):
                query_embedding = np.array(query_embedding, dtype=float)
            
            # ëª¨ë“  ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ì„œ Pythonì—ì„œ ìœ ì‚¬ë„ ê³„ì‚°
            response = self.supabase.table("document_embeddings")\
                .select("*")\
                .eq("session_id", str(self.session_id))\
                .execute()
            
            if not response.data:
                return []
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            documents_with_scores = []
            for row in response.data:
                embedding = row.get("embedding")
                
                # embeddingì´ ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                if isinstance(embedding, str):
                    try:
                        # JSON ë°°ì—´ ë¬¸ìì—´ íŒŒì‹±
                        import ast
                        embedding = ast.literal_eval(embedding)
                    except:
                        try:
                            # JSON íŒŒì‹± ì‹œë„
                            embedding = json.loads(embedding)
                        except:
                            logger.warning(f"ì„ë² ë”© íŒŒì‹± ì‹¤íŒ¨: {type(embedding)}")
                            continue
                
                # embeddingì´ ë¦¬ìŠ¤íŠ¸/ë°°ì—´ì¸ì§€ í™•ì¸
                if embedding and (isinstance(embedding, (list, tuple)) or hasattr(embedding, '__len__')):
                    try:
                        # numpy ë°°ì—´ë¡œ ë³€í™˜
                        if not isinstance(embedding, np.ndarray):
                            embedding = np.array(embedding, dtype=float)
                        
                        if len(embedding) > 0 and len(embedding) == len(query_embedding):
                            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                            query_norm = np.linalg.norm(query_embedding)
                            embed_norm = np.linalg.norm(embedding)
                            if query_norm > 0 and embed_norm > 0:
                                similarity = np.dot(query_embedding, embedding) / (query_norm * embed_norm)
                            else:
                                similarity = 0.0
                            
                            # ë©”íƒ€ë°ì´í„° íŒŒì‹±
                            metadata_str = row.get("metadata", "{}")
                            if isinstance(metadata_str, str):
                                try:
                                    metadata = json.loads(metadata_str)
                                except:
                                    metadata = {}
                            else:
                                metadata = metadata_str
                            
                            # Document ìƒì„±
                            doc = Document(
                                page_content=row.get("content", ""),
                                metadata=metadata
                            )
                            documents_with_scores.append((doc, similarity))
                        else:
                            continue
                    except Exception as e:
                        logger.warning(f"ì„ë² ë”© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                else:
                    continue
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            documents_with_scores.sort(key=lambda x: x[1], reverse=True)
            
            # ìƒìœ„ kê°œ ë°˜í™˜
            return [doc for doc, _ in documents_with_scores[:k]]
        except ImportError:
                logger.error("numpyê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install numpyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                # numpy ì—†ì´ ê°„ë‹¨í•œ ê±°ë¦¬ ê³„ì‚°
                documents = []
                for row in response.data[:k]:
                    metadata_str = row.get("metadata", "{}")
                    if isinstance(metadata_str, str):
                        try:
                            metadata = json.loads(metadata_str)
                        except:
                            metadata = {}
                    else:
                        metadata = metadata_str
                    doc = Document(
                        page_content=row.get("content", ""),
                        metadata=metadata
                    )
                    documents.append(doc)
                return documents
        except Exception as e:
            logger.error(f"ëŒ€ì²´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

# ì„¸ì…˜ ê´€ë¦¬ í•¨ìˆ˜
def save_session(session_id: str = None, user_id: str = None) -> str:
    """í˜„ì¬ ì„¸ì…˜ì„ Supabaseì— ì €ì¥"""
    try:
        # ì„¸ì…˜ ì œëª© ìë™ ìƒì„± (ì²« ë²ˆì§¸ ì§ˆë¬¸ê³¼ ë‹µë³€ ê¸°ë°˜)
        title = "ìƒˆ ì„¸ì…˜"
        if st.session_state.chat_history and len(st.session_state.chat_history) >= 2:
            first_question = st.session_state.chat_history[0].get("content", "")
            first_answer = st.session_state.chat_history[1].get("content", "")
            
            if first_question and first_answer:
                try:
                    api_keys = st.session_state.get("api_keys", {})
                    llm = get_llm(st.session_state.llm_model, temperature=0.7, api_keys=api_keys)
                    title_prompt = f"""
                    ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìš”ì•½í•˜ì—¬ ì„¸ì…˜ ì œëª©ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
                    ì œëª©ì€ ìµœëŒ€ 30ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
                    
                    ì§ˆë¬¸: {first_question[:200]}
                    ë‹µë³€: {first_answer[:300]}
                    
                    ì œëª©ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´):
                    """
                    title_response = llm.invoke(title_prompt)
                    if hasattr(title_response, 'content'):
                        title = title_response.content.strip()
                    else:
                        title = str(title_response).strip()
                    # ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                    if len(title) > 50:
                        title = title[:50]
                except Exception as e:
                    logger.warning(f"ì„¸ì…˜ ì œëª© ìƒì„± ì‹¤íŒ¨: {e}")
                    title = first_question[:30] if first_question else "ìƒˆ ì„¸ì…˜"
        
        # ì„¸ì…˜ ë°ì´í„° ì¤€ë¹„
        current_user_id = user_id or st.session_state.get("user_id")
        session_data = {
            "title": title,
            "chat_history": json.dumps(st.session_state.chat_history, ensure_ascii=False),
            "processed_files": st.session_state.processed_files,
            "llm_model": st.session_state.llm_model,
            "use_rag": st.session_state.use_rag,
            "search_model": st.session_state.search_model
        }
        
        # user_idê°€ ìˆìœ¼ë©´ ì¶”ê°€ (NULL í—ˆìš©ì´ë¯€ë¡œ ì—†ì–´ë„ ê´œì°®ìŒ)
        if current_user_id:
            session_data["user_id"] = current_user_id
        
        if session_id:
            # ê¸°ì¡´ ì„¸ì…˜ ì—…ë°ì´íŠ¸
            session_data["id"] = session_id
            response = supabase.table("sessions").update(session_data).eq("id", session_id).execute()
        else:
            # ìƒˆ ì„¸ì…˜ ìƒì„±
            response = supabase.table("sessions").insert(session_data).execute()
            session_id = response.data[0]["id"]
        
        logger.info(f"ì„¸ì…˜ ì €ì¥ ì™„ë£Œ: {session_id}")
        return session_id
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        st.error(f"ì„¸ì…˜ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def load_session(session_id: str):
    """Supabaseì—ì„œ ì„¸ì…˜ì„ ë¡œë“œ"""
    try:
        response = supabase.table("sessions").select("*").eq("id", session_id).execute()
        
        if not response.data:
            st.error("ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        session = response.data[0]
        
        # ì„¸ì…˜ ìƒíƒœ ë³µì›
        st.session_state.chat_history = json.loads(session.get("chat_history", "[]"))
        st.session_state.processed_files = session.get("processed_files", [])
        st.session_state.llm_model = session.get("llm_model", "gpt-5.1")
        st.session_state.use_rag = session.get("use_rag", False)
        st.session_state.search_model = session.get("search_model", "ì‚¬ìš© ì•ˆ í•¨")
        st.session_state.current_session_id = session_id
        
        # ë²¡í„° ìŠ¤í† ì–´ ë³µì›
        if st.session_state.processed_files:
            api_keys = st.session_state.get("api_keys", {})
            openai_key = api_keys.get("openai") or os.getenv("OPENAI_API_KEY")
            if not openai_key:
                st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False
            embeddings = OpenAIEmbeddings(api_key=openai_key)
            st.session_state.vectorstore = SupabaseVectorStore(session_id, embeddings)
            
            # ê²€ìƒ‰ê¸° ìƒì„±
            class VectorRetriever:
                def __init__(self, vectorstore: SupabaseVectorStore, k: int = 10):
                    self.vectorstore = vectorstore
                    self.k = k
                
                def invoke(self, query: str):
                    return self.vectorstore.similarity_search(query, self.k)
            
            st.session_state.retriever = VectorRetriever(st.session_state.vectorstore, k=10)
        
        logger.info(f"ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ: {session_id}")
        return True
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        st.error(f"ì„¸ì…˜ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return False

def delete_session(session_id: str):
    """ì„¸ì…˜ ì‚­ì œ"""
    try:
        # ì„¸ì…˜ê³¼ ê´€ë ¨ëœ ë²¡í„° ë°ì´í„°ë„ í•¨ê»˜ ì‚­ì œë¨ (CASCADE)
        supabase.table("sessions").delete().eq("id", session_id).execute()
        logger.info(f"ì„¸ì…˜ ì‚­ì œ ì™„ë£Œ: {session_id}")
        return True
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        st.error(f"ì„¸ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return False

def get_all_sessions(user_id: str = None) -> List[Dict]:
    """ëª¨ë“  ì„¸ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ìë³„ í•„í„°ë§)"""
    try:
        query = supabase.table("sessions").select("id, title, created_at, updated_at")
        
        if user_id:
            query = query.eq("user_id", user_id)
        else:
            # í˜„ì¬ ë¡œê·¸ì¸í•œ ì‚¬ìš©ìì˜ ì„¸ì…˜ë§Œ ê°€ì ¸ì˜¤ê¸°
            if "user_id" in st.session_state:
                query = query.eq("user_id", st.session_state.user_id)
        
        response = query.order("created_at", desc=True).execute()
        return response.data
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def get_vector_db_files(session_id: str = None) -> List[str]:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ìˆëŠ” íŒŒì¼ëª… ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        query = supabase.table("document_embeddings").select("file_name").order("file_name")
        
        if session_id:
            query = query.eq("session_id", session_id)
        else:
            if "current_session_id" in st.session_state:
                query = query.eq("session_id", st.session_state.current_session_id)
        
        response = query.execute()
        
        # ì¤‘ë³µ ì œê±°
        files = list(set([row["file_name"] for row in response.data]))
        return files
    except Exception as e:
        logger.error(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF ê¸°ë°˜ ë©€í‹°ìœ ì € ë©€í‹°ì„¸ì…˜ RAG ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide"
)

# ì´ˆê¸° ìƒíƒœ ì„¤ì •
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if "user_id" not in st.session_state:
    st.session_state.user_id = None

if "login_id" not in st.session_state:
    st.session_state.login_id = None

if "api_keys" not in st.session_state:
    st.session_state.api_keys = {}

if "conversation_memory" not in st.session_state:
    st.session_state.conversation_memory = []

if "retriever" not in st.session_state:
    st.session_state.retriever = None

if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

if "processed_files" not in st.session_state:
    st.session_state.processed_files = []

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "use_rag" not in st.session_state:
    st.session_state.use_rag = True

if "search_model" not in st.session_state:
    st.session_state.search_model = "ì‚¬ìš© ì•ˆ í•¨"

if "llm_model" not in st.session_state:
    st.session_state.llm_model = "gpt-5.1"

if "current_session_id" not in st.session_state:
    st.session_state.current_session_id = None

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
/* í—¤ë”© ìŠ¤íƒ€ì¼ */
h1 {
    font-size: 1.4rem !important;
    font-weight: 600 !important;
    color: #ff69b4 !important; /* ë¶„í™ìƒ‰ */
}
h2 {
    font-size: 1.2rem !important;
    font-weight: 600 !important;
    color: #ffd700 !important; /* ë…¸ë‘ìƒ‰ */
}
h3 {
    font-size: 1.1rem !important;
    font-weight: 600 !important;
    color: #1f77b4 !important; /* ì²­ìƒ‰ */
}
h4 {
    font-size: 1.1rem !important;
    font-weight: 600 !important;
}
h5 {
    font-size: 1rem !important;
    font-weight: 600 !important;
}
h6 {
    font-size: 0.95rem !important;
    font-weight: 600 !important;
}

/* ì±„íŒ… ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
.stChatMessage {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
}

/* ë‹µë³€ ë‚´ìš© ìŠ¤íƒ€ì¼ */
.stChatMessage p {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.5rem 0 !important;
}

/* ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ */
.stChatMessage ul, .stChatMessage ol {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.5rem 0 !important;
}

.stChatMessage li {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.3rem 0 !important;
}

/* ê°•ì¡° í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ */
.stChatMessage strong, .stChatMessage b {
    font-size: 0.95rem !important;
    font-weight: 600 !important;
}

/* ì¸ìš©ë¬¸ ìŠ¤íƒ€ì¼ */
.stChatMessage blockquote {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.5rem 0 !important;
    padding-left: 1rem !important;
    border-left: 3px solid #e0e0e0 !important;
}

/* ì½”ë“œ ìŠ¤íƒ€ì¼ */
.stChatMessage code {
    font-size: 0.9rem !important;
    background-color: #f5f5f5 !important;
    padding: 0.2rem 0.4rem !important;
    border-radius: 3px !important;
}

/* ì „ì²´ í…ìŠ¤íŠ¸ ì¼ê´€ì„± */
.stChatMessage * {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif !important;
}

/* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
.stButton > button {
    background-color: #ff69b4 !important;
    color: white !important;
    border: none !important;
    border-radius: 5px !important;
    padding: 0.5rem 1rem !important;
    font-weight: bold !important;
}

.stButton > button:hover {
    background-color: #ff1493 !important;
}
</style>
""", unsafe_allow_html=True)

# ë¡œê·¸ì¸ í˜ì´ì§€
if not st.session_state.authenticated:
    st.markdown("""
    <div style="text-align: center; margin-top: 2rem; margin-bottom: 2rem;">
        <h1 style="font-size: 3rem; font-weight: bold; margin: 0; line-height: 1.2;">
            <span style="color: #1f77b4;">PDF ê¸°ë°˜</span> 
            <span style="color: #ffd700;">ë©€í‹°ìœ ì €</span>
            <span style="color: #ff69b4;">ë©€í‹°ì„¸ì…˜</span>
            <span style="color: #1f77b4;">RAG ì±—ë´‡</span>
        </h1>
    </div>
    """, unsafe_allow_html=True)
    
    tab1, tab2 = st.tabs(["ë¡œê·¸ì¸", "íšŒì›ê°€ì…"])
    
    with tab1:
        st.markdown('<h2 style="color: #1f77b4;">ë¡œê·¸ì¸</h2>', unsafe_allow_html=True)
        login_id = st.text_input("ë¡œê·¸ì¸ ID", key="login_input")
        password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", key="password_input")
        
        if st.button("ë¡œê·¸ì¸", use_container_width=True):
            if login_id and password:
                result = authenticate_user(login_id, password)
                if result["success"]:
                    st.session_state.authenticated = True
                    st.session_state.user_id = result["user"]["id"]
                    st.session_state.login_id = result["user"]["login_id"]
                    st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
                    st.rerun()
                else:
                    st.error(result["message"])
            else:
                st.warning("ë¡œê·¸ì¸ IDì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    with tab2:
        st.markdown('<h2 style="color: #ff69b4;">íšŒì›ê°€ì…</h2>', unsafe_allow_html=True)
        new_login_id = st.text_input("ë¡œê·¸ì¸ ID", key="register_login_input")
        new_password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", key="register_password_input")
        confirm_password = st.text_input("ë¹„ë°€ë²ˆí˜¸ í™•ì¸", type="password", key="confirm_password_input")
        
        if st.button("íšŒì›ê°€ì…", use_container_width=True):
            if new_login_id and new_password and confirm_password:
                if new_password != confirm_password:
                    st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                else:
                    result = register_user(new_login_id, new_password)
                    if result["success"]:
                        st.success("íšŒì›ê°€ì… ì„±ê³µ! ë¡œê·¸ì¸ íƒ­ì—ì„œ ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
                    else:
                        st.error(result["message"])
            else:
                st.warning("ëª¨ë“  í•„ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    st.stop()

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# ì œëª© ì˜ì—­ (ìƒë‹¨ì— ë°°ì¹˜)
st.markdown("""
<div style="margin-top: -3rem; margin-bottom: 1rem;">
""", unsafe_allow_html=True)

col_title, col_user = st.columns([4, 1])

with col_title:
    # ì œëª© (ë” í¬ê²Œ)
    st.markdown("""
    <div style="text-align: center; margin-top: 0.5rem; margin-bottom: 0.5rem;">
        <h1 style="font-size: 7rem; font-weight: bold; margin: 0; line-height: 1.2;">
            <span style="color: #1f77b4;">PDF ê¸°ë°˜</span> 
            <span style="color: #ffd700;">ë©€í‹°ìœ ì €</span>
            <span style="color: #ff69b4;">ë©€í‹°ì„¸ì…˜</span>
            <span style="color: #1f77b4;">RAG ì±—ë´‡</span>
        </h1>
    </div>
    """, unsafe_allow_html=True)

with col_user:
    # ì‚¬ìš©ì ì •ë³´ ë° ë¡œê·¸ì•„ì›ƒ
    st.markdown(f"**ë¡œê·¸ì¸:** {st.session_state.login_id}")
    if st.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
        st.session_state.authenticated = False
        st.session_state.user_id = None
        st.session_state.login_id = None
        st.session_state.chat_history = []
        st.session_state.processed_files = []
        st.session_state.vectorstore = None
        st.session_state.retriever = None
        st.session_state.current_session_id = None
        st.session_state.api_keys = {}
        st.rerun()

st.markdown("</div>", unsafe_allow_html=True)

# ì„±ê³µ/ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œ (PDF ì²˜ë¦¬ í›„)
if "show_success_message" in st.session_state and st.session_state.show_success_message:
    st.success(st.session_state.show_success_message)
    # ë©”ì‹œì§€ í‘œì‹œ í›„ í”Œë˜ê·¸ ì œê±° (í•œ ë²ˆë§Œ í‘œì‹œ)
    del st.session_state.show_success_message

if "show_error_message" in st.session_state and st.session_state.show_error_message:
    st.error(st.session_state.show_error_message)
    # ë©”ì‹œì§€ í‘œì‹œ í›„ í”Œë˜ê·¸ ì œê±° (í•œ ë²ˆë§Œ í‘œì‹œ)
    del st.session_state.show_error_message

# ì²˜ë¦¬ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ìƒíƒœ í‘œì‹œ
if st.session_state.processed_files:
    st.info(f"ğŸ“„ ì²˜ë¦¬ëœ íŒŒì¼: {len(st.session_state.processed_files)}ê°œ | RAG ì‚¬ìš© ê°€ëŠ¥")
    if st.session_state.retriever:
        st.success("âœ… ë²¡í„° ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
else:
    st.markdown("ëª¨ë¸ì„ ì„ íƒí•˜ê³  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    # 0. API í‚¤ ì…ë ¥ (ìƒë‹¨)
    st.markdown('<h2 style="color: #1f77b4;">0. API í‚¤ ì„¤ì •</h2>', unsafe_allow_html=True)
    openai_key = st.text_input("OpenAI API Key", type="password", value=st.session_state.api_keys.get("openai", ""), key="openai_key_input")
    anthropic_key = st.text_input("Anthropic API Key", type="password", value=st.session_state.api_keys.get("anthropic", ""), key="anthropic_key_input")
    gemini_key = st.text_input("Google (Gemini) API Key", type="password", value=st.session_state.api_keys.get("gemini", ""), key="gemini_key_input")
    
    if st.button("API í‚¤ ì €ì¥", use_container_width=True):
        st.session_state.api_keys = {
            "openai": openai_key,
            "anthropic": anthropic_key,
            "gemini": gemini_key
        }
        st.success("API í‚¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    st.markdown("---")
    
    # 1. LLM ëª¨ë¸ ì„ íƒ
    st.markdown('<h2 style="color: #1f77b4;">1. LLM ëª¨ë¸ ì„ íƒ</h2>', unsafe_allow_html=True)
    all_models = ["gpt-5.1", "claude-sonnet-4-5", "gemini-3-pro-preview"]
    
    if 'llm_model' not in st.session_state:
        st.session_state.llm_model = all_models[0]
    
    try:
        current_index = all_models.index(st.session_state.llm_model)
    except ValueError:
        current_index = 0
    
    selected_model = st.radio(
        "ì‚¬ìš©í•  ì–¸ì–´ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
        options=all_models,
        index=current_index,
        key='llm_model_radio'
    )
    st.session_state.llm_model = selected_model

    # 2. RAG ì„ íƒ
    st.markdown('<h2 style="color: #ff69b4;">2. RAG (PDF ê²€ìƒ‰)</h2>', unsafe_allow_html=True)
    use_rag = st.radio(
        "RAGë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
        [
            "ì‚¬ìš© ì•ˆ í•¨",
            "RAG ì‚¬ìš©"
        ],
        index=0 if not st.session_state.use_rag else 1
    )
    st.session_state.use_rag = (use_rag == "RAG ì‚¬ìš©")

    # 3. PDF íŒŒì¼ ì—…ë¡œë“œ
    st.markdown('<h2 style="color: #d62728;">3. PDF íŒŒì¼ ì—…ë¡œë“œ</h2>', unsafe_allow_html=True)
    uploaded_files = st.file_uploader("PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True)
    
    if uploaded_files:
        process_button = st.button("íŒŒì¼ ì²˜ë¦¬í•˜ê¸°")
        
        if process_button:
            try:
                # ì„ì‹œ íŒŒì¼ ìƒì„± ë° ì²˜ë¦¬
                temp_dir = tempfile.TemporaryDirectory()
                
                all_docs = []
                new_files = []
                
                # ê° íŒŒì¼ ì²˜ë¦¬
                for uploaded_file in uploaded_files:
                    # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ìŠ¤í‚µ
                    if uploaded_file.name in st.session_state.processed_files:
                        continue
                        
                    temp_file_path = os.path.join(temp_dir.name, uploaded_file.name)
                    
                    # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    with open(temp_file_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    # PDF ë¡œë” ìƒì„± ë° ë¬¸ì„œ ë¡œë“œ
                    loader = PyPDFLoader(temp_file_path)
                    documents = loader.load()
                    
                    # ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ ì´ë¦„ ì¶”ê°€
                    for doc in documents:
                        doc.metadata["source"] = uploaded_file.name
                    
                    all_docs.extend(documents)
                    new_files.append(uploaded_file.name)
            
                if not all_docs:
                    st.warning("ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ retriever ì¬ìƒì„± ì‹œë„
                    if st.session_state.processed_files and st.session_state.current_session_id:
                        try:
                            openai_key = st.session_state.api_keys.get("openai") or os.getenv("OPENAI_API_KEY")
                            if not openai_key:
                                st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
                            else:
                                embeddings = OpenAIEmbeddings(api_key=openai_key)
                                if st.session_state.vectorstore is None:
                                    st.session_state.vectorstore = SupabaseVectorStore(
                                        st.session_state.current_session_id,
                                        embeddings
                                    )
                                
                                class VectorRetriever:
                                    def __init__(self, vectorstore: SupabaseVectorStore, k: int = 10):
                                        self.vectorstore = vectorstore
                                        self.k = k
                                    
                                    def invoke(self, query: str):
                                        return self.vectorstore.similarity_search(query, self.k)
                                
                                st.session_state.retriever = VectorRetriever(st.session_state.vectorstore, k=10)
                                logger.info("ê¸°ì¡´ íŒŒì¼ë¡œ retriever ì¬ìƒì„± ì™„ë£Œ")
                        except Exception as e:
                            logger.error(f"Retriever ì¬ìƒì„± ì‹¤íŒ¨: {e}")
                else:
                    with st.spinner(f"PDF íŒŒì¼ {len(new_files)}ê°œë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤... (í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘)"):
                        # í…ìŠ¤íŠ¸ ë¶„í• 
                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=500,
                            chunk_overlap=100,
                            length_function=len
                        )
                        chunks = text_splitter.split_documents(all_docs)
                        logger.info(f"ì´ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„±ë¨")
                    
                    with st.spinner("ì„ë² ë”© ìƒì„± ë° ë²¡í„° DB ì €ì¥ ì¤‘..."):
                        try:
                            # ì„ë² ë”© ìƒì„±
                            openai_key = st.session_state.api_keys.get("openai") or os.getenv("OPENAI_API_KEY")
                            if not openai_key:
                                st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
                                raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                            
                            embeddings = OpenAIEmbeddings(api_key=openai_key)
                            
                            # ì„¸ì…˜ ID í™•ì¸ ë˜ëŠ” ìƒì„±
                            if not st.session_state.current_session_id:
                                # ìƒˆ ì„¸ì…˜ì´ë©´ ë¨¼ì € ì €ì¥
                                st.session_state.current_session_id = save_session(user_id=st.session_state.user_id)
                                logger.info(f"ìƒˆ ì„¸ì…˜ ìƒì„±: {st.session_state.current_session_id}")
                            
                            # Supabase ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
                            if st.session_state.vectorstore is None:
                                st.session_state.vectorstore = SupabaseVectorStore(
                                    st.session_state.current_session_id,
                                    embeddings
                                )
                                logger.info("ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
                            
                            # ê° íŒŒì¼ë³„ë¡œ ë¬¸ì„œ ì €ì¥
                            file_chunks = {}
                            for chunk in chunks:
                                file_name = chunk.metadata.get("source", "unknown")
                                if file_name not in file_chunks:
                                    file_chunks[file_name] = []
                                file_chunks[file_name].append(chunk)
                            
                            # íŒŒì¼ë³„ë¡œ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
                            total_chunks = 0
                            for file_name, file_chunk_list in file_chunks.items():
                                logger.info(f"íŒŒì¼ {file_name} ì²˜ë¦¬ ì¤‘: {len(file_chunk_list)}ê°œ ì²­í¬")
                                st.session_state.vectorstore.add_documents(file_chunk_list, file_name)
                                total_chunks += len(file_chunk_list)
                            
                            # ê²€ìƒ‰ê¸° ìƒì„±
                            class VectorRetriever:
                                def __init__(self, vectorstore: SupabaseVectorStore, k: int = 10):
                                    self.vectorstore = vectorstore
                                    self.k = k
                                
                                def invoke(self, query: str):
                                    return self.vectorstore.similarity_search(query, self.k)
                            
                            st.session_state.retriever = VectorRetriever(st.session_state.vectorstore, k=10)
                            logger.info(f"Retriever ìƒì„± ì™„ë£Œ: {st.session_state.retriever is not None}")
                            
                            # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì—…ë°ì´íŠ¸
                            st.session_state.processed_files.extend(new_files)
                            logger.info(f"ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡: {st.session_state.processed_files}")
                            
                            # ìë™ ì €ì¥
                            session_saved = save_session(st.session_state.current_session_id, st.session_state.user_id)
                            if session_saved:
                                logger.info("ì„¸ì…˜ ì €ì¥ ì™„ë£Œ")
                            else:
                                logger.warning("ì„¸ì…˜ ì €ì¥ ì‹¤íŒ¨")
                            
                            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                            try:
                                temp_dir.cleanup()
                            except:
                                pass
                            
                            # ì„±ê³µ ë©”ì‹œì§€ (ì‚¬ì´ë“œë°”ì™€ ë©”ì¸ ì˜ì—­ ëª¨ë‘ì— í‘œì‹œ)
                            success_msg = f"âœ… {len(new_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! (ì´ {total_chunks}ê°œ ì²­í¬)"
                            st.success(success_msg)
                            logger.info(success_msg)
                            logger.info(f"ìƒíƒœ í™•ì¸ - retriever: {st.session_state.retriever is not None}, processed_files: {len(st.session_state.processed_files)}")
                            
                            # ë©”ì¸ ì˜ì—­ì—ë„ ì„±ê³µ ë©”ì‹œì§€ í‘œì‹œë¥¼ ìœ„í•œ í”Œë˜ê·¸ ì„¤ì •
                            st.session_state.show_success_message = success_msg
                            st.rerun()
                        except Exception as e:
                            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                            raise
                        
            except Exception as e:
                # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                try:
                    if 'temp_dir' in locals():
                        temp_dir.cleanup()
                except:
                    pass
                
                error_msg = f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_msg)
                logger.error(f"PDF íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)
                import traceback
                logger.error(traceback.format_exc())
                
                # ì—ëŸ¬ ë©”ì‹œì§€ë„ ë©”ì¸ ì˜ì—­ì— í‘œì‹œ
                st.session_state.show_error_message = error_msg
                st.rerun()

    # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ í‘œì‹œ
    if st.session_state.processed_files:
        st.markdown('<h3 style="color: #ffd700;">ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡</h3>', unsafe_allow_html=True)
        for file in st.session_state.processed_files:
            st.write(f"- {file}")

    # ì„¸ì…˜ ê´€ë¦¬ ì„¹ì…˜
    st.markdown('<h2 style="color: #1f77b4;">4. ì„¸ì…˜ ê´€ë¦¬</h2>', unsafe_allow_html=True)
    
    # ì„¸ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    sessions = get_all_sessions(st.session_state.user_id)
    session_titles = {s["id"]: s["title"] for s in sessions}
    
    if sessions:
        # ì„¸ì…˜ ì„ íƒ ë“œë¡­ë‹¤ìš´
        selected_session_id = st.selectbox(
            "ì„¸ì…˜ ì„ íƒ",
            options=[None] + [s["id"] for s in sessions],
            format_func=lambda x: "ìƒˆ ì„¸ì…˜" if x is None else session_titles.get(x, "ì•Œ ìˆ˜ ì—†ìŒ"),
            key="session_selector"
        )
        
        # ì„¸ì…˜ ì„ íƒ ì‹œ ìë™ ë¡œë“œ
        if selected_session_id and selected_session_id != st.session_state.get("selected_session_id"):
            st.session_state.selected_session_id = selected_session_id
            if load_session(selected_session_id):
                st.success("ì„¸ì…˜ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                st.rerun()
    else:
        st.info("ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì„¸ì…˜ ê´€ë¦¬ ë²„íŠ¼
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ì„¸ì…˜ì €ì¥", use_container_width=True):
            if st.session_state.chat_history:
                session_id = save_session(st.session_state.current_session_id, st.session_state.user_id)
                if session_id:
                    st.session_state.current_session_id = session_id
                    st.success("ì„¸ì…˜ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.rerun()
            else:
                st.warning("ì €ì¥í•  ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    with col2:
        if st.button("ì„¸ì…˜ë¡œë“œ", use_container_width=True):
            if "selected_session_id" in st.session_state and st.session_state.selected_session_id:
                if load_session(st.session_state.selected_session_id):
                    st.success("ì„¸ì…˜ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                    st.rerun()
            else:
                st.warning("ë¡œë“œí•  ì„¸ì…˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    if st.button("ì„¸ì…˜ì‚­ì œ", use_container_width=True):
        if st.session_state.current_session_id:
            if delete_session(st.session_state.current_session_id):
                st.success("ì„¸ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                # ìƒíƒœ ì´ˆê¸°í™”
                st.session_state.current_session_id = None
                st.session_state.chat_history = []
                st.session_state.processed_files = []
                st.session_state.vectorstore = None
                st.session_state.retriever = None
                st.rerun()
        else:
            st.warning("ì‚­ì œí•  ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    if st.button("í™”ë©´ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.chat_history = []
        st.session_state.conversation_memory = []
        st.rerun()
    
    if st.button("vectordb", use_container_width=True):
        files = get_vector_db_files()
        if files:
            st.markdown('<h3 style="color: #ffd700;">ë²¡í„° DB íŒŒì¼ ëª©ë¡</h3>', unsafe_allow_html=True)
            for file in files:
                st.write(f"- {file}")
        else:
            st.info("ë²¡í„° DBì— ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # í˜„ì¬ ì„¤ì • í‘œì‹œ
    st.markdown('<h3 style="color: #1f77b4;">í˜„ì¬ ì„¤ì •</h3>', unsafe_allow_html=True)
    st.text(f"ì‚¬ìš©ì: {st.session_state.login_id}")
    st.text(f"ëª¨ë¸: {st.session_state.llm_model}")
    st.text(f"RAG: {'ì‚¬ìš©' if st.session_state.use_rag else 'ì‚¬ìš© ì•ˆ í•¨'}")
    if st.session_state.processed_files:
        st.text(f"ì²˜ë¦¬ëœ íŒŒì¼: {len(st.session_state.processed_files)}ê°œ")
        st.text(f"ëŒ€í™” ê¸°ë¡: {len(st.session_state.chat_history)}ê°œ")
    if st.session_state.current_session_id:
        st.text(f"ì„¸ì…˜ ID: {st.session_state.current_session_id[:8]}...")

# ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        if isinstance(message["content"], str):
            st.markdown(message["content"])
        else:
            st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì˜ì—­
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.write(prompt)
    
    # RAG ì‚¬ìš©ì´ ì„ íƒë˜ì—ˆê³  PDF íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
    if st.session_state.use_rag and st.session_state.retriever is not None:
        with st.spinner("PDF ê¸°ë°˜ RAG ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # RAG ê²€ìƒ‰
                retrieved_docs = st.session_state.retriever.invoke(prompt)
                
                if not retrieved_docs:
                    response = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{prompt}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                else:
                    # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
                    top_docs = retrieved_docs[:3]
                    
                    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                    context_text = ""
                    max_context_length = 8000
                    current_length = 0
                    
                    for i, doc in enumerate(top_docs):
                        doc_text = f"[ë¬¸ì„œ {i+1}]\n{doc.page_content}\n\n"
                        if current_length + len(doc_text) > max_context_length:
                            break
                        context_text += doc_text
                        current_length += len(doc_text)
                    
                    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    system_prompt = f"""
                    ì§ˆë¬¸: {prompt}
                    
                    ê´€ë ¨ ë¬¸ì„œ:
                    {context_text}
                    
                    ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
                    
                    ë‹µë³€ í˜•ì‹:
                    - ë‹µë³€ì€ ë°˜ë“œì‹œ ì œëª©ê³¼ ë³¸ë¬¸ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”
                    - ì œëª©(# H1)ì€ ì§ˆë¬¸ì˜ í•µì‹¬ì„ ì§§ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš” (ìµœëŒ€ 20ì ì´ë‚´ ê¶Œì¥)
                    - ì œëª© ë‹¤ìŒì— ë¹ˆ ì¤„ì„ í•˜ë‚˜ ë‘ê³  ë³¸ë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”
                    - ë³¸ë¬¸ì€ ## (H2)ì™€ ### (H3) í—¤ë”©ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”í•˜ì„¸ìš”
                    - ë³¸ë¬¸ì€ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ë˜ ì¡´ëŒ€ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”
                    - ê°œì¡°ì‹ì´ë‚˜ ë¶ˆì™„ì „í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”
                    
                    ì£¼ì˜ì‚¬í•­:
                    - ë‹µë³€ ì¤‘ê°„ì— (ë¬¸ì„œ1), (ë¬¸ì„œ2) ê°™ì€ ì°¸ì¡° í‘œì‹œë¥¼ í•˜ì§€ ë§ˆì„¸ìš”
                    - "ì°¸ì¡° ë¬¸ì„œ:", "ì œê³µëœ ë¬¸ì„œ", "ë¬¸ì„œ 1, ë¬¸ì„œ 2" ê°™ì€ ë¬¸êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
                    - ë‹µë³€ì€ ìˆœìˆ˜í•œ ë‚´ìš©ë§Œ í¬í•¨í•˜ê³ , ì°¸ì¡° ê´€ë ¨ ë¬¸êµ¬ëŠ” ì „í˜€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
                    - ë‹µë³€ ëì— ì°¸ì¡° ì •ë³´ë‚˜ ì¶œì²˜ ê´€ë ¨ ë¬¸êµ¬ë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
                    - ë‹µë³€ ì¤‘ê°„ì— êµ¬ë¶„ì„ (---, ===, ___)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
                    - ë§ˆí¬ë‹¤ìš´ êµ¬ë¶„ì„ ì´ë‚˜ ì„ ì„ ê·¸ë¦¬ëŠ” ê¸°í˜¸ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
                    - ì·¨ì†Œì„ (~~í…ìŠ¤íŠ¸~~)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì‚­ì œëœ ë‚´ìš©ì„ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”
                    - ìˆ˜ì •ëœ ë‚´ìš©ì„ í‘œì‹œí•  ë•Œ ì·¨ì†Œì„ ì´ë‚˜ ì„ ì„ ê·¸ì–´ì„œ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”
                    """
                    
                    # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)
                    llm = get_llm(st.session_state.llm_model, temperature=1, api_keys=st.session_state.api_keys)
                    
                    response = ""
                    with st.chat_message("assistant"):
                        stream_placeholder = st.empty()
                        # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ìƒì„±
                        for chunk in llm.stream(system_prompt):
                            if hasattr(chunk, 'content'):
                                chunk_text = chunk.content
                            else:
                                chunk_text = str(chunk)
                            response += chunk_text
                            # ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ (êµ¬ë¶„ì„  ì œê±° í¬í•¨)
                            cleaned_response = remove_separators(response)
                            stream_placeholder.markdown(cleaned_response)
                    
                    # ë‹µë³€ì—ì„œ êµ¬ë¶„ì„  ì œê±°
                    response = remove_separators(response)
                
                    # ë‹¤ìŒ ì§ˆë¬¸ 3ê°œ ìƒì„±
                    next_questions_prompt = f"""
                    ì§ˆë¬¸ìê°€ í•œ ì§ˆë¬¸: {prompt}
                    
                    ìƒì„±ëœ ë‹µë³€:
                    {response}
                    
                    ìœ„ ì§ˆë¬¸ê³¼ ë‹µë³€ ë‚´ìš©ì„ ê²€í† í•˜ì—¬, ì§ˆë¬¸ìê°€ ë‹¤ìŒì— í•  ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ 3ê°€ì§€ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
                    
                    ìš”êµ¬ì‚¬í•­:
                    - ë‹µë³€ ë‚´ìš©ì„ ë” ê¹Šì´ ì´í•´í•˜ê¸° ìœ„í•œ í›„ì† ì§ˆë¬¸
                    - ë‹µë³€ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì„ êµ¬ì²´í™”í•˜ê±°ë‚˜ í™•ì¥í•˜ëŠ” ì§ˆë¬¸
                    - ê´€ë ¨ëœ ë‹¤ë¥¸ ì£¼ì œë‚˜ ê´€ì ì„ íƒìƒ‰í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
                    - ê° ì§ˆë¬¸ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±
                    - ì§ˆë¬¸ì€ ë²ˆí˜¸ ì—†ì´ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ë˜, ê° ì§ˆë¬¸ì€ ë³„ë„ì˜ ì¤„ì— ì‘ì„±
                    
                    í˜•ì‹:
                    ì§ˆë¬¸1
                    ì§ˆë¬¸2
                    ì§ˆë¬¸3
                    
                    ì°¸ê³ : ì§ˆë¬¸ë§Œ ì‘ì„±í•˜ê³ , ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
                    """
                    
                    try:
                        next_questions_response = llm.invoke(next_questions_prompt)
                        if hasattr(next_questions_response, 'content'):
                            next_questions_text = next_questions_response.content
                        else:
                            next_questions_text = str(next_questions_response)
                        
                        # ì§ˆë¬¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
                        next_questions = [q.strip() for q in next_questions_text.strip().split('\n') if q.strip() and not q.strip().startswith('#')]
                        # ìµœëŒ€ 3ê°œë§Œ ì„ íƒ
                        next_questions = next_questions[:3]
                        
                        # ë‹µë³€ ëì— ë‹¤ìŒ ì§ˆë¬¸ ì¶”ê°€
                        if next_questions:
                            response += "\n\n"
                            response += "### ğŸ’¡ ë‹¤ìŒì— ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë“¤\n\n"
                            for i, question in enumerate(next_questions, 1):
                                response += f"{i}. {question}\n\n"
                            # ë‹¤ìŒ ì§ˆë¬¸ ì¶”ê°€ í›„ ë‹¤ì‹œ í‘œì‹œ
                            with st.chat_message("assistant"):
                                st.markdown(response)
                    except Exception as e:
                        # ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ì›ë˜ ë‹µë³€ë§Œ í‘œì‹œ
                        logger.warning(f"ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
                    
                    # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                    st.session_state.chat_history.append({"role": "assistant", "content": response})
                    
                    # ëŒ€í™” ë§¥ë½ ë©”ëª¨ë¦¬ì— ì¶”ê°€
                    st.session_state.conversation_memory.append(f"ì‚¬ìš©ì: {prompt}")
                    st.session_state.conversation_memory.append(f"AI: {response}")
                    if len(st.session_state.conversation_memory) > 100:
                        st.session_state.conversation_memory = st.session_state.conversation_memory[-100:]
                    
                    # ìë™ ì €ì¥
                    if st.session_state.current_session_id:
                        save_session(st.session_state.current_session_id, st.session_state.user_id)
                
            except Exception as e:
                with st.chat_message("assistant"):
                    st.write(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                st.session_state.chat_history.append({"role": "assistant", "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"})
                logger.error(f"RAG ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")

    # RAG ì‚¬ìš©ì´ ì„ íƒë˜ì§€ ì•Šì•˜ê±°ë‚˜ PDF íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
    else:
        if st.session_state.use_rag and st.session_state.retriever is None:
            # ë””ë²„ê¹… ì •ë³´ ë¡œê¹…
            logger.warning(f"RAG ì„ íƒë˜ì—ˆìœ¼ë‚˜ retrieverê°€ Noneì…ë‹ˆë‹¤.")
            logger.warning(f"ìƒíƒœ í™•ì¸ - use_rag: {st.session_state.use_rag}, retriever: {st.session_state.retriever}, processed_files: {st.session_state.processed_files}, vectorstore: {st.session_state.vectorstore is not None}")
            
            with st.chat_message("assistant"):
                if st.session_state.processed_files:
                    st.warning(f"íŒŒì¼ì€ ì²˜ë¦¬ë˜ì—ˆì§€ë§Œ ê²€ìƒ‰ê¸°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì²˜ë¦¬ëœ íŒŒì¼: {st.session_state.processed_files}")
                    st.info("ğŸ’¡ íŒŒì¼ì„ ë‹¤ì‹œ ì²˜ë¦¬í•˜ê±°ë‚˜ í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
                else:
                    st.warning("RAGë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            st.session_state.chat_history.append({"role": "assistant", "content": "RAGë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”."})
            logger.warning("RAG ì„ íƒë˜ì—ˆìœ¼ë‚˜ PDF íŒŒì¼ì´ ì—†ìŒ")
        else:
            try:
                llm = get_llm(st.session_state.llm_model, temperature=1, api_keys=st.session_state.api_keys)
                direct_prompt = f"""ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {prompt}

ë‹µë³€ í˜•ì‹:
- ë‹µë³€ì€ ë°˜ë“œì‹œ ì œëª©ê³¼ ë³¸ë¬¸ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”
- ì œëª©(# H1)ì€ ì§ˆë¬¸ì˜ í•µì‹¬ì„ ì§§ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš” (ìµœëŒ€ 20ì ì´ë‚´ ê¶Œì¥)
- ì œëª© ë‹¤ìŒì— ë¹ˆ ì¤„ì„ í•˜ë‚˜ ë‘ê³  ë³¸ë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”
- ë³¸ë¬¸ì€ ## (H2)ì™€ ### (H3) í—¤ë”©ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”í•˜ì„¸ìš”
- ë³¸ë¬¸ì€ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ë˜ ì¡´ëŒ€ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ê°œì¡°ì‹ì´ë‚˜ ë¶ˆì™„ì „í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”

ì£¼ì˜ì‚¬í•­:
- ë‹µë³€ ì¤‘ê°„ì— êµ¬ë¶„ì„ (---, ===, ___)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ë§ˆí¬ë‹¤ìš´ êµ¬ë¶„ì„ ì´ë‚˜ ì„ ì„ ê·¸ë¦¬ëŠ” ê¸°í˜¸ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì·¨ì†Œì„ (~~í…ìŠ¤íŠ¸~~)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì‚­ì œëœ ë‚´ìš©ì„ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”
- ìˆ˜ì •ëœ ë‚´ìš©ì„ í‘œì‹œí•  ë•Œ ì·¨ì†Œì„ ì´ë‚˜ ì„ ì„ ê·¸ì–´ì„œ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”"""
                
                response = ""
                with st.chat_message("assistant"):
                    stream_placeholder = st.empty()
                    # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ìƒì„±
                    for chunk in llm.stream(direct_prompt):
                        if hasattr(chunk, 'content'):
                            chunk_text = chunk.content
                        else:
                            chunk_text = str(chunk)
                        response += chunk_text
                        # ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ (êµ¬ë¶„ì„  ì œê±° í¬í•¨)
                        cleaned_response = remove_separators(response)
                        stream_placeholder.markdown(cleaned_response)
                
                # ë‹µë³€ì—ì„œ êµ¬ë¶„ì„  ì œê±°
                response = remove_separators(response)
                
                # ë‹¤ìŒ ì§ˆë¬¸ 3ê°œ ìƒì„±
                try:
                    next_questions_prompt = f"""
                    ì§ˆë¬¸ìê°€ í•œ ì§ˆë¬¸: {prompt}
                    
                    ìƒì„±ëœ ë‹µë³€:
                    {response}
                    
                    ìœ„ ì§ˆë¬¸ê³¼ ë‹µë³€ ë‚´ìš©ì„ ê²€í† í•˜ì—¬, ì§ˆë¬¸ìê°€ ë‹¤ìŒì— í•  ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ 3ê°€ì§€ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
                    
                    ìš”êµ¬ì‚¬í•­:
                    - ë‹µë³€ ë‚´ìš©ì„ ë” ê¹Šì´ ì´í•´í•˜ê¸° ìœ„í•œ í›„ì† ì§ˆë¬¸
                    - ë‹µë³€ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì„ êµ¬ì²´í™”í•˜ê±°ë‚˜ í™•ì¥í•˜ëŠ” ì§ˆë¬¸
                    - ê´€ë ¨ëœ ë‹¤ë¥¸ ì£¼ì œë‚˜ ê´€ì ì„ íƒìƒ‰í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
                    - ê° ì§ˆë¬¸ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±
                    - ì§ˆë¬¸ì€ ë²ˆí˜¸ ì—†ì´ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ë˜, ê° ì§ˆë¬¸ì€ ë³„ë„ì˜ ì¤„ì— ì‘ì„±
                    
                    í˜•ì‹:
                    ì§ˆë¬¸1
                    ì§ˆë¬¸2
                    ì§ˆë¬¸3
                    
                    ì°¸ê³ : ì§ˆë¬¸ë§Œ ì‘ì„±í•˜ê³ , ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
                    """
                    next_questions_response = llm.invoke(next_questions_prompt)
                    if hasattr(next_questions_response, 'content'):
                        next_questions_text = next_questions_response.content
                    else:
                        next_questions_text = str(next_questions_response)
                    
                    next_questions = [q.strip() for q in next_questions_text.strip().split('\n') if q.strip() and not q.strip().startswith('#')]
                    next_questions = next_questions[:3]
                    
                    if next_questions:
                        response += "\n\n"
                        response += "### ğŸ’¡ ë‹¤ìŒì— ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë“¤\n\n"
                        for i, question in enumerate(next_questions, 1):
                            response += f"{i}. {question}\n\n"
                        # ë‹¤ìŒ ì§ˆë¬¸ ì¶”ê°€ í›„ ë‹¤ì‹œ í‘œì‹œ
                        with st.chat_message("assistant"):
                            st.markdown(response)
                except Exception as e:
                    logger.warning(f"ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
                
                st.session_state.chat_history.append({"role": "assistant", "content": response})
                
                # ìë™ ì €ì¥
                if st.session_state.current_session_id:
                    save_session(st.session_state.current_session_id, st.session_state.user_id)
                    
            except Exception as e:
                error_message = f"LLM ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
                st.error(error_message)
                st.session_state.chat_history.append({"role": "assistant", "content": error_message})
                logger.error(f"LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")

